{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "import re\n",
    "def clean_text(text):\n",
    "\n",
    "    text = text.replace(\"\\n\", \"\")\n",
    "\n",
    "    cleaned_text = re.sub('[a-zA-Z]', '', text)\n",
    "\n",
    "    cleaned_text = re.sub('[\\{\\}\\[\\]\\/?.,;:|\\)*~`!^\\-_+<>@\\#$%&\\\\\\=\\(\\'\\\"]',\n",
    "\n",
    "    '', cleaned_text)\n",
    "\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_image(title, main_url):\n",
    "    \n",
    "    res = requests.get(main_url)\n",
    "    html = res.text\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    img_urls = []\n",
    "    for img_url in soup.select('img[src$=.jpg]'):\n",
    "        img_urls.append(img_url['src'])\n",
    "\n",
    "    # 디렉토리 생성\n",
    "    title = clean_text(title)\n",
    "    dir_name = title\n",
    "    if not os.path.isdir(dir_name):\n",
    "        os.mkdir(dir_name)\n",
    "\n",
    "    print(len(img_urls)) \n",
    "    for img_url in img_urls:\n",
    "        #print(img_url)\n",
    "        req_header = {\n",
    "            'referer':main_url\n",
    "        }\n",
    "        res2 = requests.get(img_url,headers=req_header)\n",
    "        img_data = res2.content\n",
    "        file_name = os.path.basename(img_url)\n",
    "        with open(dir_name+'/'+file_name, 'wb') as file:\n",
    "            file.write(img_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_page(title, main_url):\n",
    "    url = main_url\n",
    "    \n",
    "    res = requests.get(url)\n",
    "    html = res.text\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    webtoon_list = []\n",
    "    webtoon_dict = {}\n",
    "    \n",
    "    for main_ti in soup.select('.detail h2'):\n",
    "            main_title = main_ti.text.split()\n",
    "    length = len(main_title)\n",
    "    main_title = main_title[0:length-1]\n",
    "    pre_dir = ' '.join(main_title)\n",
    "\n",
    "    dir_name = 'img/'+pre_dir\n",
    "    os.chdir('C:\\\\mypython\\\\python_study\\\\data')\n",
    "    if not os.path.isdir(dir_name):\n",
    "        os.mkdir(dir_name)\n",
    "\n",
    "    for a_tag in soup.select('#content table a'):\n",
    "        title = a_tag.text.strip()\n",
    "        link = urljoin(main_url,a_tag['href'])\n",
    "        webtoon_dict = {'title': title, 'link': link}\n",
    "        if webtoon_dict['title'] != '':\n",
    "            webtoon_list.append(webtoon_dict)\n",
    "    main_title = soup.select('#content h2')\n",
    "    for webtoon in webtoon_list:\n",
    "        os.chdir('C:\\\\mypython\\\\python_study\\\\data'+'\\\\'+dir_name)\n",
    "        write_image(webtoon['title'], webtoon['link'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "35\n",
      "33\n",
      "36\n",
      "35\n",
      "34\n",
      "36\n",
      "38\n",
      "39\n",
      "39\n",
      "36\n",
      "34\n",
      "36\n",
      "36\n",
      "40\n",
      "43\n",
      "40\n",
      "35\n",
      "37\n",
      "39\n",
      "38\n",
      "57\n",
      "101\n",
      "81\n",
      "75\n",
      "79\n",
      "78\n",
      "90\n",
      "92\n",
      "92\n",
      "76\n",
      "0\n",
      "53\n",
      "53\n",
      "53\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "import re\n",
    "\n",
    "main_url = 'https://comic.naver.com/index.nhn'\n",
    "\n",
    "res = requests.get(main_url)\n",
    "html = res.text\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "webtoon_list = []\n",
    "webtoon_dict = {}\n",
    "soup.select('#genreRecommand h6 a')\n",
    "for a_tag in soup.select('#genreRecommand h6 a'):\n",
    "    title = a_tag.text.strip()\n",
    "    link = urljoin(main_url,a_tag['href'])\n",
    "    webtoon_dict = {'title': title, 'link': link}\n",
    "    webtoon_list.append(webtoon_dict)\n",
    "\n",
    "for webtoon in webtoon_list:\n",
    "    next_page(webtoon['title'], webtoon['link'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
